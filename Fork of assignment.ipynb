{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exercise-1', 'bertpretrained']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from keras.layers import Dense\n",
    "#splitting data into test-train \n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "import re\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#Data loading\n",
    "data = pd.read_csv(\"../input/exercise-1/text_emotion.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting column from data frame into list\n",
    "y = data[\"sentiment\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[\"content\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_train split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#label encoding   \n",
    "le = LabelEncoder() \n",
    "ytrain_enc = le.fit_transform(ytrain)\n",
    "ytest_enc = le.transform(ytest)\n",
    "ytrain_enc = keras.utils.to_categorical(ytrain_enc, num_classes=13)\n",
    "ytest_enc = keras.utils.to_categorical(ytest_enc, num_classes=13)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(ytest_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install keras-bert library\n",
    "!pip install -q keras-bert\n",
    "import keras_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "SEQ_LEN = 32\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "LR = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "import os\n",
    "\n",
    "pretrained_path = '../input/bertpretrained/uncased_l-24_h-1024_a-16/uncased_L-24_H-1024_A-16/'\n",
    "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
    "checkpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
    "vocab_path = os.path.join(pretrained_path, 'vocab.txt')\n",
    "\n",
    "# TF_KERAS must be added to environment variables in order to use TPU\n",
    "os.environ['TF_KERAS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Basic Model\n",
    "import codecs\n",
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "\n",
    "token_dict = {}\n",
    "with codecs.open(vocab_path, 'r', 'utf8') as reader:\n",
    "    for line in reader:\n",
    "        token = line.strip()\n",
    "        token_dict[token] = len(token_dict)\n",
    "\n",
    "model = load_trained_model_from_checkpoint(\n",
    "    config_path,\n",
    "    checkpoint_path,\n",
    "    training=True,\n",
    "    trainable=True,\n",
    "    seq_len=SEQ_LEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:01<00:00, 5639.49it/s]\n",
      "100%|██████████| 32000/32000 [00:05<00:00, 5652.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing of the data and Conversion of Data to Array\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras_bert import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(token_dict)    \n",
    "\n",
    "def preprocess_tweet(tweet):  #Removing special characters and digits\n",
    "    words = tweet.split(\" \")\n",
    "    new_list = []\n",
    "    for word in words:\n",
    "        if \"@\" in word or \"http\" in word:\n",
    "            pass\n",
    "        else:\n",
    "            new_list.append(word)\n",
    "    new_tweet = \" \".join(new_list)\n",
    "    tweet1 = re.sub(r\"@[^\\s]+[\\s]?\",'',new_tweet)\n",
    "    tweet3 = re.sub('[0-9]', '', tweet1)\n",
    "    return tweet3\n",
    "\n",
    "def load_data(x):   # Defining the function\n",
    "    global tokenizer \n",
    "    indices = [] \n",
    "    for text in tqdm(x):   #For content in a list, apply for loop\n",
    "        text = preprocess_tweet(text)\n",
    "        ids,segments = tokenizer.encode(text, max_len=SEQ_LEN)\n",
    "        indices.append(ids)\n",
    "        \n",
    "    indices = np.array(indices) #creating an array of indices\n",
    "    return [indices, np.zeros_like(indices)]#returning two arrays,one of indices \n",
    "                                           #and a similar one of zeros  \n",
    "\n",
    "#Applying function on xtrain and xtest (lists of content)       \n",
    "test_x = load_data(xtest)   \n",
    "train_x = load_data(xtrain)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Custom Model\n",
    "import keras\n",
    "from keras_bert import AdamWarmup, calc_train_steps\n",
    "from keras.layers import Dense\n",
    "\n",
    "inputs = model.inputs[:2]\n",
    "dense = model.get_layer('NSP-Dense').output\n",
    "outputs = Dense(units=13, activation='softmax')(dense)\n",
    "\n",
    "decay_steps, warmup_steps = calc_train_steps(\n",
    "    ytrain_enc.shape[0],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=10,\n",
    ")\n",
    "\n",
    "model = keras.models.Model(inputs, outputs)\n",
    "model.compile(\n",
    "    AdamWarmup(decay_steps=decay_steps, warmup_steps=warmup_steps, lr=LR),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Variables\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "sess = K.get_session()\n",
    "uninitialized_variables = set([i.decode('ascii') \n",
    "for i in sess.run(tf.report_uninitialized_variables())])\n",
    "    init_op = tf.variables_initializer(\n",
    "    [v for v in tf.global_variables() if v.name.split(':')[0] in uninitialized_variables]\n",
    ")\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "32000/32000 [==============================] - 453s 14ms/step - loss: 2.5972 - acc: 0.1293\n",
      "Epoch 2/5\n",
      "32000/32000 [==============================] - 416s 13ms/step - loss: 2.0678 - acc: 0.2869\n",
      "Epoch 3/5\n",
      "32000/32000 [==============================] - 416s 13ms/step - loss: 1.9529 - acc: 0.3331\n",
      "Epoch 4/5\n",
      "32000/32000 [==============================] - 416s 13ms/step - loss: 1.8930 - acc: 0.3536\n",
      "Epoch 5/5\n",
      "32000/32000 [==============================] - 416s 13ms/step - loss: 1.8616 - acc: 0.3656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb693ab5b70>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with tf.keras.utils.custom_object_scope(get_custom_objects()):\n",
    "        \n",
    "model.fit(\n",
    "        train_x,\n",
    "        ytrain_enc,\n",
    "        epochs = 5,\n",
    "        batch_size = BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(\n",
    "        test_x,\n",
    "        ytest_enc,\n",
    "        batch_size = BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.lr = 5e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
